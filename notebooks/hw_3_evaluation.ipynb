{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必須課題(1) 動作の確認\n",
    "> このページで用いた検索結果に対するpyNTCIREVALの出力のうち, MSnDCG@0003とnERR@0003が\n",
    "> 講義資料の定義に従った計算と一致していることを確かめよ.\n",
    "> つまり, nDCG@3とnERR@3を計算するプログラムを書き,\n",
    "> その結果がpyNTCIREVALの結果と一致していることを確認せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from operator import add, mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [START 講義で扱ったnDCGとnERRの実装]\n",
    "def DCG(rels):\n",
    "    \"\"\"DCGの計算\n",
    "    \n",
    "    params\n",
    "    ---------\n",
    "    rels : list, i番目の要素はi番目の文書の適合度を表す.\n",
    "    \"\"\"\n",
    "    elems = [(2 ** rel - 1) / np.log2(1 + (idx + 1)) for idx, rel in enumerate(rels)]\n",
    "    return reduce(add, elems)\n",
    "\n",
    "def ERR(rels):\n",
    "    MAX_REL = 2.0\n",
    "    stop_proba = lambda rel: (2 ** rel - 1) / (2 ** MAX_REL)\n",
    "    ans = 0.0\n",
    "    continue_probas = [1.0 - stop_proba(rel) for rel in rels]\n",
    "    for idx, rel in enumerate(rels):\n",
    "        p_stop = stop_proba(rel)\n",
    "        rec = 1.0 if idx == 0 else reduce(mul, continue_probas[:idx])\n",
    "        ans += (p_stop * rec) / (idx + 1)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [START 文書と関連度の読み込み] pandas が扱いやすい\n",
    "df = pd.read_csv(\"../data/eval/q1.rel\", header=None, sep=\" \")\n",
    "df.columns = [\"doc\", \"rel\"]\n",
    "res = [\"d1\", \"d2\", \"d3\"]\n",
    "gain_map = { \"L0\": 0, \"L1\": 1, \"L2\": 2 } # 授業ではL2: 3だが、計算が合わない...\n",
    "df[\"score\"] = df[\"rel\"].map(lambda rel: gain_map[rel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG_obs: 2.5\n",
      "DCG_opt: 4.130929753571458\n",
      "nDCG@3: 0.6052\n"
     ]
    }
   ],
   "source": [
    "# [START nDCG@3の計算]\n",
    "rels = df[df[\"doc\"].isin([\"d1\", \"d2\", \"d3\"])][\"score\"].tolist()\n",
    "opt_rels = df[\"score\"].sort_values(ascending=False).tolist()[0:3]\n",
    "dcg_obs = DCG(rels)\n",
    "dcg_opt = DCG(opt_rels)\n",
    "\n",
    "print(\"DCG_obs: {}\".format(dcg_obs))\n",
    "print(\"DCG_opt: {}\".format(dcg_opt))\n",
    "\n",
    "ndcg = dcg_obs / dcg_opt\n",
    "\n",
    "print(\"nDCG@3: {}\".format(round(ndcg, 4))) # 丸め込むことで一致する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR_obs: 0.4375\n",
      "ERR_opt: 0.796875\n",
      "nERR@3: 0.549\n"
     ]
    }
   ],
   "source": [
    "# [START ERR@3の計算]\n",
    "err_obs = ERR(rels)\n",
    "err_opt = ERR(opt_rels)\n",
    "nerr = err_obs / err_opt\n",
    "\n",
    "print(\"ERR_obs: {}\".format(err_obs))\n",
    "print(\"ERR_opt: {}\".format(err_opt))\n",
    "print(\"nERR@3: {}\".format(round(nerr, 4))) # 丸め込むことで一致する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確かに一致していることがわかる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必須課題(2) 独自データに対する評価指標の計算\n",
    "> 演習課題1で扱った検索課題集合と検索結果に対して各自で評価用データを作成し\n",
    "> pyNTCIREVALを用いて評価指標を計算せよ.\n",
    "> そして, MRR, nDCG@3及びnERR@3の平均を報告し,\n",
    "> それらの値の違いが各指標のどういった要因によるものかを考察せよ.\n",
    "> なお, 演習課題1で扱ったコーパス以外で評価データを作成しても良い.\n",
    "> ただし, 評価データはダミーデータではなく, 実際の何らかのランキングを評価したものとし,\n",
    "> 検索課題(クエリ)は3つ以上とする.\n",
    "\n",
    "+ 文書の適合度は上位k件について自分で決めれば良い\n",
    "    + k = 10くらいが好ましい?\n",
    "+ MRRはRRの平均値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1の上位3件と関連度\n",
      "d33 L2\n",
      "d14 L1\n",
      "d25 L1\n",
      "q2の上位3件と関連度\n",
      "d6 L0\n",
      "d17 L1\n",
      "d71 L1\n",
      "q3の上位3件と関連度\n",
      "d62 L0\n",
      "d15 L0\n",
      "d32 L1\n"
     ]
    }
   ],
   "source": [
    "# [START 評価用データの作成]\n",
    "# q1 = {\"京都\", \"紅葉\"}\n",
    "!pyNTCIREVAL label -r ../data/hw/q1.rel < ../data/hw/q1.res > ../data/hw/label.q1.rel\n",
    "print(\"q1の上位3件と関連度\")\n",
    "!cat ../data/hw/label.q1.rel\n",
    "# q2 = {\"銀閣寺\", \"場所\"}\n",
    "!pyNTCIREVAL label -r ../data/hw/q2.rel < ../data/hw/q2.res > ../data/hw/label.q2.rel\n",
    "print(\"q2の上位3件と関連度\")\n",
    "!cat ../data/hw/label.q2.rel\n",
    "# q3 = {\"鴨川\", \"料理\"}\n",
    "!pyNTCIREVAL label -r ../data/hw/q3.rel < ../data/hw/q3.res > ../data/hw/label.q3.rel\n",
    "print(\"q3の上位3件と関連度\")\n",
    "!cat ../data/hw/label.q3.rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [START 評価指標の計算結果の書き出し]\n",
    "!pyNTCIREVAL compute -r ../data/hw/q1.rel -g 1:3 --cutoffs=1,3 < ../data/hw/label.q1.rel > ../data/hw/q1.eval\n",
    "!pyNTCIREVAL compute -r ../data/hw/q2.rel -g 1:3 --cutoffs=1,3 < ../data/hw/label.q2.rel > ../data/hw/q2.eval\n",
    "!pyNTCIREVAL compute -r ../data/hw/q3.rel -g 1:3 --cutoffs=1,3 < ../data/hw/label.q3.rel > ../data/hw/q3.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RR=                 1.0000\n",
      " ERR=                0.7969\n",
      " nERR@0001=          1.0000\n",
      " nERR@0003=          0.9273\n",
      "------------------------------------------------------------------------------------------\n",
      " RR=                 0.5000\n",
      " ERR=                0.1875\n",
      " nERR@0001=          0.0000\n",
      " nERR@0003=          0.2353\n",
      "------------------------------------------------------------------------------------------\n",
      " RR=                 0.3333\n",
      " ERR=                0.0833\n",
      " nERR@0001=          0.0000\n",
      " nERR@0003=          0.2133\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/hw/q1.eval | grep RR\n",
    "print(\"---\"*30)\n",
    "!cat ../data/hw/q2.eval | grep RR\n",
    "print(\"---\"*30)\n",
    "!cat ../data/hw/q3.eval | grep RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.6111\n"
     ]
    }
   ],
   "source": [
    "# [START MRRの計算]\n",
    "print(\"MRR: {}\".format(np.mean([1.0, 0.5, 0.3333])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nERR@3: 0.4577\n"
     ]
    }
   ],
   "source": [
    "# [START ERRの平均値の計算]\n",
    "print(\"nERR@3: {}\".format(np.mean([0.9273, 0.2325, 0.2133])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSnDCG@0003=        0.6462\n",
      "------------------------------------------------------------------------------------------\n",
      " MSnDCG@0003=        0.2738\n",
      "------------------------------------------------------------------------------------------\n",
      " MSnDCG@0003=        0.2346\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/hw/q1.eval | grep MSnDCG@0003\n",
    "print(\"---\" * 30)\n",
    "!cat ../data/hw/q2.eval | grep MSnDCG@0003\n",
    "print(\"---\" * 30)\n",
    "!cat ../data/hw/q3.eval | grep MSnDCG@0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@3: 0.38486666666666663\n"
     ]
    }
   ],
   "source": [
    "# [START nDCGの平均値の計算]\n",
    "print(\"nDCG@3: {}\".format(np.mean([0.6462, 0.2738, 0.2346])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各指標の値のばらつきに対する考察\n",
    "計算された結果を次のようにまとめる.\n",
    "\n",
    "| 指標 | 平均 | q1 | q2 | q3 |\n",
    "|:------|:------|:----|:----|:----|\n",
    "| MRR | 0.611 | 1.000 | 0.500 | 0.333 |\n",
    "| nDCG@3 | 0.384 | 0.646 | 0.273 | 0.234 |\n",
    "| nERR@3 | 0.457 | 0.927 | 0.235 | 0.213 |\n",
    "\n",
    "### MRR\n",
    "`MRR`は次のような式で表現される評価指標である.\n",
    "$$MRR = \\frac{1}{\\|Q|\\|}\\sum_{i=1}^{\\|Q\\|}\\frac{1}{rank_{i}}$$\n",
    "$rank_{i}$は, ランキングの結果上位から最初に関連度のある文書の順位を意味する.\n",
    "\n",
    "クエリq1, q2, q3についてのランクづけ結果において, それぞれ1, 2, 3番目に初めて関連のある文書が現れている. \n",
    "したがって, 計算されたMRRの値から, 大抵1番目か2番目までに少なくとも関連のある文書が現れるような検索を実現できていると言える.\n",
    "\n",
    "また, MRRの導出において考慮される関連のある文書の数はたかだか1つであり, それぞれの検索結果について3番目までには関連のある文書が現れているので, その他の指標の値に比べて高い値となった.\n",
    "\n",
    "この性質から, MRRはその他の指標と比較して, 計算するためのコストが小さい評価指標であると言える.\n",
    "なぜならば, 何らかの検索結果について, 上位から「関連のある」文書を一つ見つけるだけでよく, 判断回数が少なく, かつ関連度合まで考慮する必要がないからである."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nDCG@3 & nERR@3\n",
    "nDCG, nERRは, MRRと比較して文書の適合「度」を考慮する評価指標であり, それぞれの式は次のように表現される.\n",
    "$$\n",
    "nDCG@k = \\frac{DCG@k_{obs}}{DCG@k_{opt}}\n",
    "$$\n",
    "$$\n",
    "nERR@k = \\frac{ERR@k_{obs}}{ERR@k_{opt}}\n",
    "$$\n",
    "ただし\n",
    "$$\n",
    "DCG@k = \\sum_{i=1}^{k}\\frac{2^{rel_{i}} - 1}{log_{2}(i+1)}\n",
    "$$\n",
    "$$\n",
    "ERR@k = \\sum_{i=1}^{k}\\frac{1}{i}\\prod_{j}^{i - 1}(1 - R_{j})R_{i}\n",
    "$$\n",
    "\n",
    "これら2つの指標を比較して, nERRは上位に適合度が大きい文書があるほどそれよりも下位の文書が指標に与える影響が小さいと言える.\n",
    "\n",
    "q1は関連度の高い文書が上位に現れているため, nERR@3の値は0.93程度と非常に高いが, nDCGについては, 10番目までに関連度が`L2`であるような文書が他に存在するため0.64程度と小さい値になっている.\n",
    "このことから, nERRは検索結果の上位に存在する関連度の大きな文書に影響を受けやすい一方で, nDCGは検索結果に含まれる文書全体の関連度の高さに影響を受けやすい指標であることがわかる.\n",
    "\n",
    "#### 注意\n",
    "nDCGとnERRについての考察は, q2, q3については検索結果がさほど好ましくなく, それぞれの指標の特徴があまり現れていないためq1に注目して考えた."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [python360]",
   "language": "python",
   "name": "Python [python360]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
